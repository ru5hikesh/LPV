{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence1 = \"It's true, Ms. Martha Jones! #Truth\"\n",
    "sentence2 = \"I played the play playfully as the players were playing in the play with playfullness\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace tokenization = [\"It's\", 'true,', 'Ms.', 'Martha', 'Jones!', '#Truth']\n",
      "Punctuation-based tokenization = ['It', \"'\", 's', 'true', ',', 'Ms', '.', 'Martha', 'Jones', '!', '#', 'Truth']\n",
      "Multi-word expression (MWE) tokenization = ['It', \"'s\", 'true', ',', 'Ms.', 'Martha_Jones', '!', '#', 'Truth']\n",
      "Tweet-rules based tokenization = [\"It's\", 'true', ',', 'Ms', '.', 'Martha', 'Jones', '!', '#Truth']\n",
      "Default/Treebank tokenization = ['It', \"'s\", 'true', ',', 'Ms.', 'Martha', 'Jones', '!', '#', 'Truth']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import (\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    TreebankWordTokenizer,\n",
    "    TweetTokenizer,\n",
    "    MWETokenizer\n",
    ")\n",
    "\n",
    "print(f'Whitespace tokenization = {sentence1.split()}')\n",
    "\n",
    "print(f'Punctuation-based tokenization = {wordpunct_tokenize(sentence1)}')\n",
    "\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe(('Martha', 'Jones'))\n",
    "print(f'Multi-word expression (MWE) tokenization = {tokenizer.tokenize(word_tokenize(sentence1))}')\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "print(f'Tweet-rules based tokenization = {tokenizer.tokenize(sentence1)}')\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(f'Default/Treebank tokenization = {tokenizer.tokenize(sentence1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['It', \"'s\", 'true', ',', 'Ms.', 'Martha', 'Jones', '!', '#', 'Truth']\n",
      "\n",
      "Tokenized sentences: [\"It's true, Ms. Martha Jones!\", '#Truth']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "print('Tokenized words:', word_tokenize(sentence1))\n",
    "print('\\nTokenized sentences:', sent_tokenize(sentence1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I             --> \t i\n",
      "played        --> \t play\n",
      "the           --> \t the\n",
      "play          --> \t play\n",
      "playfully     --> \t play\n",
      "as            --> \t as\n",
      "the           --> \t the\n",
      "players       --> \t player\n",
      "were          --> \t were\n",
      "playing       --> \t play\n",
      "in            --> \t in\n",
      "the           --> \t the\n",
      "play          --> \t play\n",
      "with          --> \t with\n",
      "playfullness  --> \t playful\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#list of tokenized words\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "#stem's of each word\n",
    "stem_words = [stemmer.stem(word) for word in token]\n",
    "\n",
    "#print stemming results\n",
    "for e1, e2 in zip(token, stem_words):\n",
    "    print(e1.ljust(13), '-->', '\\t', e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I             --> \t i\n",
      "played        --> \t play\n",
      "the           --> \t the\n",
      "play          --> \t play\n",
      "playfully     --> \t play\n",
      "as            --> \t as\n",
      "the           --> \t the\n",
      "players       --> \t player\n",
      "were          --> \t were\n",
      "playing       --> \t play\n",
      "in            --> \t in\n",
      "the           --> \t the\n",
      "play          --> \t play\n",
      "with          --> \t with\n",
      "playfullness  --> \t playful\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#the stemmer requires a language parameter\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "#list of tokenized words\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "#stem's of each word\n",
    "stem_words = [snow_stemmer.stem(word) for word in token]\n",
    "\n",
    "#print stemming results\n",
    "for e1, e2 in zip(token, stem_words):\n",
    "    print(e1.ljust(13), '-->', '\\t', e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I             --> \t I\n",
      "played        --> \t played\n",
      "the           --> \t the\n",
      "play          --> \t play\n",
      "playfully     --> \t playfully\n",
      "as            --> \t a\n",
      "the           --> \t the\n",
      "players       --> \t player\n",
      "were          --> \t were\n",
      "playing       --> \t playing\n",
      "in            --> \t in\n",
      "the           --> \t the\n",
      "play          --> \t play\n",
      "with          --> \t with\n",
      "playfullness  --> \t playfullness\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "token = word_tokenize(sentence2)\n",
    "\n",
    "lemmatized_output = [lemmatizer.lemmatize(word) for word in token]\n",
    "\n",
    "#print stemming results\n",
    "for e1, e2 in zip(token, lemmatized_output):\n",
    "    print(e1.ljust(13), '-->', '\\t', e2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
